{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardiac Arrhythmia Multy-Class Classification \n",
    "\n",
    "Analyze data and address missing data if there is any. \n",
    "\n",
    "Decide aboute a good evaluation strategy and justify your choice. \n",
    "\n",
    "Find the best parameters for the following classification models: \n",
    "- KNN classifcation \n",
    "- Logistic Regression\n",
    "- Linear Supprt Vector Machine\n",
    "- Kerenilzed Support Vector Machine\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "\n",
    "Then use different bagging and boosting methods to boost the results? Do you see any significant change? Why or why not? \n",
    "\n",
    "Next, use data reduction method you have learned in class to reduce the size of data, and agian try above models. Do you get better results? Justify your answer. \n",
    "\n",
    "<font color = 'red'>Due date for full credit: April 4, 11:59 PM\n",
    "    <br>\n",
    "    Due date for partial credit: April 6, 11:59 PM.\n",
    "    <br> No submission will be accepted after April 6. \n",
    "    <br> Please note that your term paper is also due April 6. \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('cardiac_arrhythmia.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "      <td>167</td>\n",
       "      <td>321</td>\n",
       "      <td>174</td>\n",
       "      <td>91</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>52</td>\n",
       "      <td>77</td>\n",
       "      <td>129</td>\n",
       "      <td>377</td>\n",
       "      <td>133</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>54</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>157</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "      <td>118</td>\n",
       "      <td>354</td>\n",
       "      <td>160</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>30.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>67</td>\n",
       "      <td>89</td>\n",
       "      <td>130</td>\n",
       "      <td>383</td>\n",
       "      <td>156</td>\n",
       "      <td>73</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20.1</td>\n",
       "      <td>25.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>72</td>\n",
       "      <td>102</td>\n",
       "      <td>135</td>\n",
       "      <td>401</td>\n",
       "      <td>156</td>\n",
       "      <td>83</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>86</td>\n",
       "      <td>77</td>\n",
       "      <td>143</td>\n",
       "      <td>373</td>\n",
       "      <td>150</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>58</td>\n",
       "      <td>78</td>\n",
       "      <td>155</td>\n",
       "      <td>382</td>\n",
       "      <td>163</td>\n",
       "      <td>81</td>\n",
       "      <td>-24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>73</td>\n",
       "      <td>91</td>\n",
       "      <td>180</td>\n",
       "      <td>355</td>\n",
       "      <td>157</td>\n",
       "      <td>104</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>48.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>88</td>\n",
       "      <td>77</td>\n",
       "      <td>158</td>\n",
       "      <td>399</td>\n",
       "      <td>163</td>\n",
       "      <td>94</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>39.2</td>\n",
       "      <td>54.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>48</td>\n",
       "      <td>75</td>\n",
       "      <td>132</td>\n",
       "      <td>350</td>\n",
       "      <td>169</td>\n",
       "      <td>65</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17.2</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>59</td>\n",
       "      <td>82</td>\n",
       "      <td>145</td>\n",
       "      <td>347</td>\n",
       "      <td>169</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>19.5</td>\n",
       "      <td>41.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>58</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>353</td>\n",
       "      <td>122</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>17.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>154</td>\n",
       "      <td>392</td>\n",
       "      <td>175</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>82</td>\n",
       "      <td>181</td>\n",
       "      <td>399</td>\n",
       "      <td>158</td>\n",
       "      <td>79</td>\n",
       "      <td>-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>25.2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>58</td>\n",
       "      <td>83</td>\n",
       "      <td>251</td>\n",
       "      <td>383</td>\n",
       "      <td>189</td>\n",
       "      <td>183</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>17.1</td>\n",
       "      <td>54.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>336</td>\n",
       "      <td>177</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>132</td>\n",
       "      <td>364</td>\n",
       "      <td>169</td>\n",
       "      <td>82</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>19.7</td>\n",
       "      <td>34.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>59</td>\n",
       "      <td>75</td>\n",
       "      <td>157</td>\n",
       "      <td>406</td>\n",
       "      <td>143</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>55</td>\n",
       "      <td>82</td>\n",
       "      <td>140</td>\n",
       "      <td>388</td>\n",
       "      <td>149</td>\n",
       "      <td>82</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.3</td>\n",
       "      <td>57.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>80</td>\n",
       "      <td>109</td>\n",
       "      <td>128</td>\n",
       "      <td>382</td>\n",
       "      <td>195</td>\n",
       "      <td>60</td>\n",
       "      <td>-34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20.7</td>\n",
       "      <td>29.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>73</td>\n",
       "      <td>94</td>\n",
       "      <td>186</td>\n",
       "      <td>373</td>\n",
       "      <td>224</td>\n",
       "      <td>125</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>68.4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>161</td>\n",
       "      <td>407</td>\n",
       "      <td>168</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>54.8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>164</td>\n",
       "      <td>420</td>\n",
       "      <td>381</td>\n",
       "      <td>99</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>96</td>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "      <td>301</td>\n",
       "      <td>82</td>\n",
       "      <td>-37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>57</td>\n",
       "      <td>83</td>\n",
       "      <td>164</td>\n",
       "      <td>359</td>\n",
       "      <td>154</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>56.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>203</td>\n",
       "      <td>367</td>\n",
       "      <td>171</td>\n",
       "      <td>106</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>21</td>\n",
       "      <td>140</td>\n",
       "      <td>157</td>\n",
       "      <td>438</td>\n",
       "      <td>226</td>\n",
       "      <td>81</td>\n",
       "      <td>-40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>36.7</td>\n",
       "      <td>115.9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>150</td>\n",
       "      <td>362</td>\n",
       "      <td>177</td>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>52.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>163</td>\n",
       "      <td>340</td>\n",
       "      <td>162</td>\n",
       "      <td>102</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>20.9</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>65</td>\n",
       "      <td>133</td>\n",
       "      <td>148</td>\n",
       "      <td>417</td>\n",
       "      <td>260</td>\n",
       "      <td>92</td>\n",
       "      <td>-158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>63</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>23.7</td>\n",
       "      <td>26.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>24</td>\n",
       "      <td>77</td>\n",
       "      <td>125</td>\n",
       "      <td>358</td>\n",
       "      <td>159</td>\n",
       "      <td>70</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>16.1</td>\n",
       "      <td>49.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>29</td>\n",
       "      <td>123</td>\n",
       "      <td>145</td>\n",
       "      <td>361</td>\n",
       "      <td>221</td>\n",
       "      <td>80</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>12.2</td>\n",
       "      <td>25.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>56</td>\n",
       "      <td>79</td>\n",
       "      <td>145</td>\n",
       "      <td>381</td>\n",
       "      <td>173</td>\n",
       "      <td>101</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>20.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>42</td>\n",
       "      <td>88</td>\n",
       "      <td>123</td>\n",
       "      <td>362</td>\n",
       "      <td>228</td>\n",
       "      <td>81</td>\n",
       "      <td>-18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>84.4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>149</td>\n",
       "      <td>290</td>\n",
       "      <td>128</td>\n",
       "      <td>93</td>\n",
       "      <td>-67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>65</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>360</td>\n",
       "      <td>163</td>\n",
       "      <td>71</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>62</td>\n",
       "      <td>79</td>\n",
       "      <td>155</td>\n",
       "      <td>367</td>\n",
       "      <td>153</td>\n",
       "      <td>95</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>24.1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "      <td>77</td>\n",
       "      <td>144</td>\n",
       "      <td>340</td>\n",
       "      <td>148</td>\n",
       "      <td>82</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>19.1</td>\n",
       "      <td>36.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>63</td>\n",
       "      <td>87</td>\n",
       "      <td>142</td>\n",
       "      <td>391</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>43.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>146</td>\n",
       "      <td>357</td>\n",
       "      <td>179</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>30.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>170</td>\n",
       "      <td>383</td>\n",
       "      <td>152</td>\n",
       "      <td>115</td>\n",
       "      <td>92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>72</td>\n",
       "      <td>88</td>\n",
       "      <td>153</td>\n",
       "      <td>389</td>\n",
       "      <td>172</td>\n",
       "      <td>89</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>24.9</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>74</td>\n",
       "      <td>143</td>\n",
       "      <td>374</td>\n",
       "      <td>146</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>40.1</td>\n",
       "      <td>55.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>143</td>\n",
       "      <td>363</td>\n",
       "      <td>146</td>\n",
       "      <td>84</td>\n",
       "      <td>-40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>157</td>\n",
       "      <td>384</td>\n",
       "      <td>132</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>63</td>\n",
       "      <td>81</td>\n",
       "      <td>143</td>\n",
       "      <td>325</td>\n",
       "      <td>218</td>\n",
       "      <td>74</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>75</td>\n",
       "      <td>91</td>\n",
       "      <td>134</td>\n",
       "      <td>376</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>17.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>57</td>\n",
       "      <td>81</td>\n",
       "      <td>151</td>\n",
       "      <td>363</td>\n",
       "      <td>166</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>199</td>\n",
       "      <td>382</td>\n",
       "      <td>154</td>\n",
       "      <td>117</td>\n",
       "      <td>-37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>137</td>\n",
       "      <td>361</td>\n",
       "      <td>201</td>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>62.4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>68</td>\n",
       "      <td>108</td>\n",
       "      <td>176</td>\n",
       "      <td>365</td>\n",
       "      <td>194</td>\n",
       "      <td>116</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>-28.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-44.2</td>\n",
       "      <td>-33.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>106</td>\n",
       "      <td>386</td>\n",
       "      <td>218</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>46.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>70</td>\n",
       "      <td>79</td>\n",
       "      <td>127</td>\n",
       "      <td>364</td>\n",
       "      <td>138</td>\n",
       "      <td>78</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>21.3</td>\n",
       "      <td>32.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   270   271   272  \\\n",
       "0     75    0  190   80   91  193  371  174  121  -16 ...   0.0   9.0  -0.9   \n",
       "1     56    1  165   64   81  174  401  149   39   25 ...   0.0   8.5   0.0   \n",
       "2     54    0  172   95  138  163  386  185  102   96 ...   0.0   9.5  -2.4   \n",
       "3     55    0  175   94  100  202  380  179  143   28 ...   0.0  12.2  -2.2   \n",
       "4     75    0  190   80   88  181  360  177  103  -16 ...   0.0  13.1  -3.6   \n",
       "5     13    0  169   51  100  167  321  174   91  107 ...  -0.6  12.2  -2.8   \n",
       "6     40    1  160   52   77  129  377  133   77   77 ...   0.0   6.5   0.0   \n",
       "7     49    1  162   54   78    0  376  157   70   67 ...   0.0   8.2  -1.9   \n",
       "8     44    0  168   56   84  118  354  160   63   61 ...   0.0   7.0  -1.3   \n",
       "9     50    1  167   67   89  130  383  156   73   85 ...  -0.6  10.8  -1.7   \n",
       "10    62    0  170   72  102  135  401  156   83   72 ...  -0.5   9.0  -2.0   \n",
       "11    45    1  165   86   77  143  373  150   65   12 ...   0.0   4.4  -2.2   \n",
       "12    54    1  172   58   78  155  382  163   81  -24 ...   0.0   6.3  -2.1   \n",
       "13    30    0  170   73   91  180  355  157  104   68 ...  -0.9  12.3   0.0   \n",
       "14    44    1  160   88   77  158  399  163   94   46 ...  -0.6  12.4   0.0   \n",
       "15    47    1  150   48   75  132  350  169   65   36 ...   0.0   7.7  -0.8   \n",
       "16    47    0  171   59   82  145  347  169   61   77 ...   0.0   9.4  -1.7   \n",
       "17    46    1  158   58   70  120  353  122   52   57 ...   0.0   6.6   0.0   \n",
       "18    73    0  165   63   91  154  392  175   83   73 ...   0.0   5.7   0.0   \n",
       "19    57    1  166   72   82  181  399  158   79  -12 ...   0.0   7.7  -0.9   \n",
       "20    28    1  160   58   83  251  383  189  183   50 ...  -0.6   9.1  -1.4   \n",
       "21    45    0  169   67   90  122  336  177   78   81 ...  -0.6   8.3  -1.8   \n",
       "22    36    1  153   75   71  132  364  169   82   62 ...   0.0   8.9  -1.0   \n",
       "23    57    1  165   59   75  157  406  143   92    4 ...   0.0   6.7  -0.5   \n",
       "24    40    1  153   55   82  140  388  149   82   52 ...   0.0  13.6   0.0   \n",
       "25    44    0  169   80  109  128  382  195   60  -34 ...   0.0   6.9   0.0   \n",
       "26    34    0  170   73   94  186  373  224  125   90 ...   0.0  15.3  -1.1   \n",
       "27    31    1  160   54   95  161  407  168   83   10 ...   0.0  12.7  -1.8   \n",
       "28    56    1  164   65   90  164  420  381   99   -8 ...   0.0   5.4   0.0   \n",
       "29    51    1  160   83   96  147  400  301   82  -37 ...   0.0   7.3  -3.9   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...   ...   ...   \n",
       "422   29    1  162   57   83  164  359  154   69   64 ...   0.0  14.1  -2.2   \n",
       "423   51    0  186   95   94  203  367  171  106   -7 ...   0.0   9.6  -3.5   \n",
       "424    7    0  119   21  140  157  438  226   81  -40 ...   0.0  10.0  -2.1   \n",
       "425   36    0  171   93   87  150  362  177   96   44 ...   0.0  10.3  -0.8   \n",
       "426   35    1  160   53   55  163  340  162  102   40 ...   0.0   8.7  -0.5   \n",
       "427   58    0  160   65  133  148  417  260   92 -158 ...  -0.4   6.4  -3.5   \n",
       "428   64    0  160   63   83    0  364  120   90   29 ...   0.0   6.7  -0.4   \n",
       "429    8    1  130   24   77  125  358  159   70   87 ...   0.0  11.3  -2.1   \n",
       "430   11    0  138   29  123  145  361  221   80  112 ...  -3.4  19.6  -4.2   \n",
       "431   47    0  166   56   79  145  381  173  101   52 ...   0.0   8.5   0.0   \n",
       "432   11    0  140   42   88  123  362  228   81  -18 ...   0.0  17.1  -7.1   \n",
       "433   70    0  167   60   80  149  290  128   93  -67 ...   0.0   2.7  -5.4   \n",
       "434   20    0  178   65   88  155  360  163   71  -22 ...  -0.5  10.2   0.0   \n",
       "435   39    1  164   62   79  155  367  153   95   50 ...   0.0   9.7  -0.7   \n",
       "436   32    1  164   57   77  144  340  148   82   27 ...  -0.6   9.9  -0.6   \n",
       "437   35    1  155   63   87  142  391  137   88   66 ...   0.0  10.7   0.0   \n",
       "438   37    0  175   82   88  146  357  179   72    1 ...  -0.4  13.5  -1.2   \n",
       "439   49    1  168   66   94  170  383  152  115   92 ...   0.0   8.2  -0.7   \n",
       "440   37    0  176   72   88  153  389  172   89   67 ...  -0.9  16.6  -3.4   \n",
       "441   37    1  160   50   74  143  374  146   75   68 ...   0.0  11.4  -0.9   \n",
       "442   65    1  160   50   85  143  363  146   84  -40 ...   0.0   6.6  -6.1   \n",
       "443   41    1  154   75   88  157  384  132  112   65 ...  -0.4  10.5  -2.5   \n",
       "444   29    0  166   63   81  143  325  218   74   24 ...   0.0   7.8  -1.3   \n",
       "445   45    0  175   75   91  134  376  160   83   91 ...   0.0   7.1  -2.4   \n",
       "446   20    1  157   57   81  151  363  166   80   43 ...   0.0   7.2  -0.7   \n",
       "447   53    1  160   70   80  199  382  154  117  -37 ...   0.0   4.3  -5.0   \n",
       "448   37    0  190   85  100  137  361  201   73   86 ...   0.0  15.6  -1.6   \n",
       "449   36    0  166   68  108  176  365  194  116  -85 ...   0.0  16.3 -28.6   \n",
       "450   32    1  155   55   93  106  386  218   63   54 ...  -0.4  12.0  -0.7   \n",
       "451   78    1  160   70   79  127  364  138   78   28 ...   0.0  10.4  -1.8   \n",
       "\n",
       "     273 274  275  276   277    278  279  \n",
       "0    0.0   0  0.9  2.9  23.3   49.4    8  \n",
       "1    0.0   0  0.2  2.1  20.4   38.8    6  \n",
       "2    0.0   0  0.3  3.4  12.3   49.0   10  \n",
       "3    0.0   0  0.4  2.6  34.6   61.6    1  \n",
       "4    0.0   0 -0.1  3.9  25.4   62.8    7  \n",
       "5    0.0   0  0.9  2.2  13.5   31.1   14  \n",
       "6    0.0   0  0.4  1.0  14.3   20.5    1  \n",
       "7    0.0   0  0.1  0.5  15.8   19.8    1  \n",
       "8    0.0   0  0.6  2.1  12.5   30.9    1  \n",
       "9    0.0   0  0.8  0.9  20.1   25.1   10  \n",
       "10   0.0   0  0.8  0.9  12.3   19.3    3  \n",
       "11   0.0   0  0.5  1.5   4.9   17.2    1  \n",
       "12   0.0   0  0.8  0.5   8.8   12.1   10  \n",
       "13   0.0   0  0.4  2.1  28.5   48.6    6  \n",
       "14   0.0   0  0.3  1.7  39.2   54.1    1  \n",
       "15   0.0   0  0.6  1.7  17.2   31.1    1  \n",
       "16   0.0   0  0.6  2.3  19.5   41.1   10  \n",
       "17   0.0   0  0.3  0.7  17.1   20.8    1  \n",
       "18   0.0   0  0.4  0.5  18.2   22.4    1  \n",
       "19   0.0   0  0.5  1.8  25.2   38.5    1  \n",
       "20   0.0   0  0.6  3.3  17.1   54.7    1  \n",
       "21   0.0   0  0.8  1.1  11.7   19.6    1  \n",
       "22   0.0   0  0.5  1.7  19.7   34.3    1  \n",
       "23   0.0   0  0.4  1.1  18.4   28.9    1  \n",
       "24   0.0   0  0.5  2.5  35.3   57.3    1  \n",
       "25   0.0   0  0.4  1.3  20.7   29.2   16  \n",
       "26   0.0   0  0.6  2.6  44.0   68.4   14  \n",
       "27   0.0   0  0.3  3.2  25.4   54.8   10  \n",
       "28   0.0   0  0.4 -1.4  17.2    3.0    2  \n",
       "29   0.0   0  0.5 -1.1   3.6   -6.3    2  \n",
       "..   ...  ..  ...  ...   ...    ...  ...  \n",
       "422  0.0   0  0.5  3.0  32.7   56.1    1  \n",
       "423  0.0   0  1.0  1.6   9.4   23.4    1  \n",
       "424  0.0   0  1.0  5.5  36.7  115.9    9  \n",
       "425  0.0   0  0.6  3.0  24.1   52.9    1  \n",
       "426  0.0   0  0.5  2.3  20.9   40.6    1  \n",
       "427  0.0   0  0.4  0.8  -2.9    6.5   10  \n",
       "428  0.0   0  0.3  0.4  23.7   26.4    1  \n",
       "429  0.0   0  0.7  3.6  16.1   49.2   16  \n",
       "430  0.0   0  0.2  1.8  12.2   25.1   10  \n",
       "431  0.0   0  0.6  1.2  20.4   29.0    6  \n",
       "432  0.0   0  0.7  5.5  15.1   84.4   10  \n",
       "433  0.0   0  0.3 -0.2  -7.1   -8.3    3  \n",
       "434  0.0   0  0.5  0.4  24.0   25.4    1  \n",
       "435  0.0   0  0.8  1.3  24.1   33.7    1  \n",
       "436  0.0   0  0.5  2.4  19.1   36.3    1  \n",
       "437  0.0   0  1.0  2.1  25.6   43.2    1  \n",
       "438  0.0   0  0.5  0.6  30.1   35.0    1  \n",
       "439  0.0   0  0.8  1.7  21.5   33.7    1  \n",
       "440  0.0   0  0.7  1.8  24.9   41.4    1  \n",
       "441  0.0   0  0.7  1.8  40.1   55.5    1  \n",
       "442  0.0   0  0.5  0.5  -3.8    0.4    1  \n",
       "443  0.0   0  0.5  1.4  17.8   29.5   10  \n",
       "444  0.0   0  0.5  2.3  14.1   37.1    1  \n",
       "445  0.0   0 -0.4  1.3   8.5   17.6    1  \n",
       "446  0.0   0  0.5  2.3  17.6   39.2    1  \n",
       "447  0.0   0  0.7  0.6  -4.4   -0.5    1  \n",
       "448  0.0   0  0.4  2.4  38.0   62.4   10  \n",
       "449  0.0   0  1.5  1.0 -44.2  -33.2    2  \n",
       "450  0.0   0  0.5  2.4  25.0   46.6    1  \n",
       "451  0.0   0  0.5  1.6  21.3   32.8    1  \n",
       "\n",
       "[452 rows x 280 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the instances of group which is having just two members for better cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= data[(data[279] !=8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.replace('?',np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       8\n",
       "11      22\n",
       "12       1\n",
       "13     375\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "      ... \n",
       "250      0\n",
       "251      0\n",
       "252      0\n",
       "253      0\n",
       "254      0\n",
       "255      0\n",
       "256      0\n",
       "257      0\n",
       "258      0\n",
       "259      0\n",
       "260      0\n",
       "261      0\n",
       "262      0\n",
       "263      0\n",
       "264      0\n",
       "265      0\n",
       "266      0\n",
       "267      0\n",
       "268      0\n",
       "269      0\n",
       "270      0\n",
       "271      0\n",
       "272      0\n",
       "273      0\n",
       "274      0\n",
       "275      0\n",
       "276      0\n",
       "277      0\n",
       "278      0\n",
       "279      0\n",
       "Length: 280, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.DataFrameGroupBy object at 0x1a124f70f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group = data.groupby([279])\n",
    "data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279\n",
       "1     245\n",
       "2      44\n",
       "3      15\n",
       "4      15\n",
       "5      13\n",
       "6      25\n",
       "7       3\n",
       "9       9\n",
       "10     50\n",
       "14      4\n",
       "15      5\n",
       "16     22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= data_group.transform(lambda  grp: grp.fillna(grp.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X.loc[:, ~(X == 0).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  15,  16,  17,\n",
       "        18,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,\n",
       "        32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,\n",
       "        45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n",
       "        58,  59,  60,  61,  62,  63,  64,  65,  66,  68,  70,  71,  72,\n",
       "        73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  84,  85,  86,\n",
       "        87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "       100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "       113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "       126, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 140, 142,\n",
       "       144, 146, 147, 148, 149, 150, 152, 153, 154, 155, 158, 159, 160,\n",
       "       161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174,\n",
       "       175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
       "       188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
       "       201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
       "       215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
       "       228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
       "       241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267,\n",
       "       268, 269, 270, 271, 272, 273, 275, 276, 277, 278])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = X.columns.values\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X[cols] = scaler.fit_transform(X[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.195489</td>\n",
       "      <td>0.332061</td>\n",
       "      <td>0.610108</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.190244</td>\n",
       "      <td>0.577713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558747</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.360169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31250</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.500971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099259</td>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.311069</td>\n",
       "      <td>0.555957</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.497561</td>\n",
       "      <td>0.785924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583812</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402542</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>0.566990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.385496</td>\n",
       "      <td>0.534296</td>\n",
       "      <td>0.260073</td>\n",
       "      <td>0.697561</td>\n",
       "      <td>0.586510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687206</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.592481</td>\n",
       "      <td>0.648544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.345420</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.502439</td>\n",
       "      <td>0.457478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587467</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.523308</td>\n",
       "      <td>0.656311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.318702</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.443902</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588512</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53125</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.433835</td>\n",
       "      <td>0.451133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.481928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>0.246183</td>\n",
       "      <td>0.523466</td>\n",
       "      <td>0.091575</td>\n",
       "      <td>0.375610</td>\n",
       "      <td>0.730205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430809</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.382524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.590361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.084444</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.172932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.700880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441253</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.933566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.377994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.218045</td>\n",
       "      <td>0.225191</td>\n",
       "      <td>0.440433</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.307317</td>\n",
       "      <td>0.683284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585379</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.449838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.091852</td>\n",
       "      <td>0.358824</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.545126</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.356098</td>\n",
       "      <td>0.753666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486684</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.483459</td>\n",
       "      <td>0.412298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.353383</td>\n",
       "      <td>0.257634</td>\n",
       "      <td>0.610108</td>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.404878</td>\n",
       "      <td>0.715543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351436</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>0.374757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.542169</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.509025</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.539589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361880</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.369173</td>\n",
       "      <td>0.361165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.650602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099259</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.172932</td>\n",
       "      <td>0.295802</td>\n",
       "      <td>0.541516</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.395122</td>\n",
       "      <td>0.434018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374935</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.328155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.343511</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.507317</td>\n",
       "      <td>0.703812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749347</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.521186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.546617</td>\n",
       "      <td>0.564401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.530120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>0.301527</td>\n",
       "      <td>0.602888</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.639296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574413</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.627068</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.566265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.425993</td>\n",
       "      <td>0.223443</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.609971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548825</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326271</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.461654</td>\n",
       "      <td>0.451133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097778</td>\n",
       "      <td>0.311765</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.223443</td>\n",
       "      <td>0.297561</td>\n",
       "      <td>0.730205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578068</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.515858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.554217</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.078519</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.112782</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.436823</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.253659</td>\n",
       "      <td>0.671554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434465</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.460902</td>\n",
       "      <td>0.384466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.335294</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>0.577617</td>\n",
       "      <td>0.245421</td>\n",
       "      <td>0.404878</td>\n",
       "      <td>0.718475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467363</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.469173</td>\n",
       "      <td>0.394822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090370</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.345420</td>\n",
       "      <td>0.602888</td>\n",
       "      <td>0.183150</td>\n",
       "      <td>0.385366</td>\n",
       "      <td>0.469208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559791</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326271</td>\n",
       "      <td>0.968531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.521805</td>\n",
       "      <td>0.499029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.337349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.479008</td>\n",
       "      <td>0.545126</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.892683</td>\n",
       "      <td>0.651026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669974</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.460902</td>\n",
       "      <td>0.603883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>0.358824</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.232824</td>\n",
       "      <td>0.375451</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.380488</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445953</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.351695</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.420301</td>\n",
       "      <td>0.376699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>0.405882</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>0.223443</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.686217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529504</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.480451</td>\n",
       "      <td>0.471845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.311765</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.628159</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.448780</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480418</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.470677</td>\n",
       "      <td>0.436893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.481928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071111</td>\n",
       "      <td>0.288235</td>\n",
       "      <td>0.203008</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.563177</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.656891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633943</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.597744</td>\n",
       "      <td>0.620712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094815</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.244275</td>\n",
       "      <td>0.541516</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.404692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448042</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.487970</td>\n",
       "      <td>0.438835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>0.354962</td>\n",
       "      <td>0.509025</td>\n",
       "      <td>0.424908</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.768328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844909</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.648305</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.663158</td>\n",
       "      <td>0.692557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.373494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.307252</td>\n",
       "      <td>0.631769</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.404878</td>\n",
       "      <td>0.533724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631854</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538136</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.523308</td>\n",
       "      <td>0.604531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.347059</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482927</td>\n",
       "      <td>0.480938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312794</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.461654</td>\n",
       "      <td>0.269256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.614458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.452941</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.280534</td>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.706960</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.395894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246997</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.309322</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.359398</td>\n",
       "      <td>0.209061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.635379</td>\n",
       "      <td>0.234432</td>\n",
       "      <td>0.443902</td>\n",
       "      <td>0.351906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493473</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.396992</td>\n",
       "      <td>0.355340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.349398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.084444</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.458484</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.336585</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631854</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597458</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.578195</td>\n",
       "      <td>0.612945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>0.387405</td>\n",
       "      <td>0.487365</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.517073</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427676</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.877622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>0.401294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.639098</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.743682</td>\n",
       "      <td>0.432234</td>\n",
       "      <td>0.395122</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761880</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.608271</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097778</td>\n",
       "      <td>0.511765</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.286260</td>\n",
       "      <td>0.469314</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.633431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647520</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436441</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.513534</td>\n",
       "      <td>0.592233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.421687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.276471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311069</td>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.497561</td>\n",
       "      <td>0.621701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643342</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368644</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.489474</td>\n",
       "      <td>0.512621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.347059</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.282443</td>\n",
       "      <td>0.667870</td>\n",
       "      <td>0.556777</td>\n",
       "      <td>0.448780</td>\n",
       "      <td>0.041056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345692</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.877622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.310526</td>\n",
       "      <td>0.291909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.335294</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.589443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451697</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283898</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.510526</td>\n",
       "      <td>0.420712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.096386</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>0.238550</td>\n",
       "      <td>0.454874</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.759531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571802</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478814</td>\n",
       "      <td>0.926573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.453383</td>\n",
       "      <td>0.568285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048889</td>\n",
       "      <td>0.135294</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.465704</td>\n",
       "      <td>0.413919</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.832845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488773</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31250</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.424060</td>\n",
       "      <td>0.412298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090370</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.180451</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.537906</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.492683</td>\n",
       "      <td>0.656891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551958</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.360169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.437540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.234733</td>\n",
       "      <td>0.469314</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.395122</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724576</td>\n",
       "      <td>0.751748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.445865</td>\n",
       "      <td>0.796117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091852</td>\n",
       "      <td>0.317647</td>\n",
       "      <td>0.187970</td>\n",
       "      <td>0.284351</td>\n",
       "      <td>0.209386</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.453659</td>\n",
       "      <td>0.307918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248042</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.278947</td>\n",
       "      <td>0.196117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108148</td>\n",
       "      <td>0.347059</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.295802</td>\n",
       "      <td>0.462094</td>\n",
       "      <td>0.201465</td>\n",
       "      <td>0.346341</td>\n",
       "      <td>0.439883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337859</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.432203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.512782</td>\n",
       "      <td>0.414239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.469880</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.180451</td>\n",
       "      <td>0.295802</td>\n",
       "      <td>0.487365</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.651026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527937</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411017</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.513534</td>\n",
       "      <td>0.467961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.385542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.165414</td>\n",
       "      <td>0.274809</td>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.583578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591123</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.419492</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.475940</td>\n",
       "      <td>0.484790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.421687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.335294</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.270992</td>\n",
       "      <td>0.574007</td>\n",
       "      <td>0.106227</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>0.697947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515927</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.524812</td>\n",
       "      <td>0.529450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.278626</td>\n",
       "      <td>0.451264</td>\n",
       "      <td>0.260073</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>0.507331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548303</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.558647</td>\n",
       "      <td>0.476375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.590361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>0.324427</td>\n",
       "      <td>0.545126</td>\n",
       "      <td>0.161172</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492428</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347458</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.493985</td>\n",
       "      <td>0.467961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105185</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.291985</td>\n",
       "      <td>0.566787</td>\n",
       "      <td>0.234432</td>\n",
       "      <td>0.434146</td>\n",
       "      <td>0.700880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613055</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.519549</td>\n",
       "      <td>0.517799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.512635</td>\n",
       "      <td>0.139194</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.703812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.724282</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.968531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.633835</td>\n",
       "      <td>0.609061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0.783133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.472924</td>\n",
       "      <td>0.139194</td>\n",
       "      <td>0.409756</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345170</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.786713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.303759</td>\n",
       "      <td>0.252427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.493976</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.072593</td>\n",
       "      <td>0.405882</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.548736</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.546341</td>\n",
       "      <td>0.695015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634987</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.912587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.466165</td>\n",
       "      <td>0.440777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090370</td>\n",
       "      <td>0.335294</td>\n",
       "      <td>0.195489</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.335740</td>\n",
       "      <td>0.402930</td>\n",
       "      <td>0.360976</td>\n",
       "      <td>0.574780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593734</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.438346</td>\n",
       "      <td>0.489968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.405882</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.255725</td>\n",
       "      <td>0.519856</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.404878</td>\n",
       "      <td>0.771261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425065</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300847</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.396241</td>\n",
       "      <td>0.363754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.240964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.077037</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.195489</td>\n",
       "      <td>0.288168</td>\n",
       "      <td>0.472924</td>\n",
       "      <td>0.212454</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.630499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587990</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.464662</td>\n",
       "      <td>0.503560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.187970</td>\n",
       "      <td>0.379771</td>\n",
       "      <td>0.541516</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.570732</td>\n",
       "      <td>0.395894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328982</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46875</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.246602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.261450</td>\n",
       "      <td>0.465704</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.356098</td>\n",
       "      <td>0.756598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762924</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.618045</td>\n",
       "      <td>0.653722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090370</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.335878</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>0.315018</td>\n",
       "      <td>0.565854</td>\n",
       "      <td>0.255132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.385542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.288235</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.202290</td>\n",
       "      <td>0.555957</td>\n",
       "      <td>0.402930</td>\n",
       "      <td>0.307317</td>\n",
       "      <td>0.662757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650653</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.975524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.520301</td>\n",
       "      <td>0.551456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.939759</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.180451</td>\n",
       "      <td>0.242366</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.380488</td>\n",
       "      <td>0.586510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535248</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.492481</td>\n",
       "      <td>0.462136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows Ã— 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1         2         3         4         5         6    \\\n",
       "1    0.674699  1.0  0.088889  0.341176  0.195489  0.332061  0.610108   \n",
       "2    0.650602  0.0  0.099259  0.523529  0.624060  0.311069  0.555957   \n",
       "3    0.662651  0.0  0.103704  0.517647  0.338346  0.385496  0.534296   \n",
       "4    0.903614  0.0  0.125926  0.435294  0.248120  0.345420  0.462094   \n",
       "5    0.156627  0.0  0.094815  0.264706  0.338346  0.318702  0.321300   \n",
       "6    0.481928  1.0  0.081481  0.270588  0.165414  0.246183  0.523466   \n",
       "7    0.590361  1.0  0.084444  0.282353  0.172932  0.000000  0.519856   \n",
       "8    0.530120  0.0  0.093333  0.294118  0.218045  0.225191  0.440433   \n",
       "9    0.602410  1.0  0.091852  0.358824  0.255639  0.248092  0.545126   \n",
       "10   0.746988  0.0  0.096296  0.388235  0.353383  0.257634  0.610108   \n",
       "11   0.542169  1.0  0.088889  0.470588  0.165414  0.272901  0.509025   \n",
       "12   0.650602  1.0  0.099259  0.305882  0.172932  0.295802  0.541516   \n",
       "13   0.361446  0.0  0.096296  0.394118  0.270677  0.343511  0.444043   \n",
       "14   0.530120  1.0  0.081481  0.482353  0.165414  0.301527  0.602888   \n",
       "15   0.566265  1.0  0.066667  0.247059  0.150376  0.251908  0.425993   \n",
       "16   0.566265  0.0  0.097778  0.311765  0.203008  0.276718  0.415162   \n",
       "17   0.554217  1.0  0.078519  0.305882  0.112782  0.229008  0.436823   \n",
       "18   0.879518  0.0  0.088889  0.335294  0.270677  0.293893  0.577617   \n",
       "19   0.686747  1.0  0.090370  0.388235  0.203008  0.345420  0.602888   \n",
       "20   0.337349  1.0  0.081481  0.305882  0.210526  0.479008  0.545126   \n",
       "21   0.542169  0.0  0.094815  0.358824  0.263158  0.232824  0.375451   \n",
       "22   0.433735  1.0  0.071111  0.405882  0.120301  0.251908  0.476534   \n",
       "23   0.686747  1.0  0.088889  0.311765  0.150376  0.299618  0.628159   \n",
       "24   0.481928  1.0  0.071111  0.288235  0.203008  0.267176  0.563177   \n",
       "25   0.530120  0.0  0.094815  0.435294  0.406015  0.244275  0.541516   \n",
       "26   0.409639  0.0  0.096296  0.394118  0.293233  0.354962  0.509025   \n",
       "27   0.373494  1.0  0.081481  0.282353  0.300752  0.307252  0.631769   \n",
       "28   0.674699  1.0  0.087407  0.347059  0.263158  0.312977  0.678700   \n",
       "29   0.614458  1.0  0.081481  0.452941  0.308271  0.280534  0.606498   \n",
       "30   0.638554  0.0  0.103704  0.464706  0.225564  0.299618  0.635379   \n",
       "..        ...  ...       ...       ...       ...       ...       ...   \n",
       "422  0.349398  1.0  0.084444  0.300000  0.210526  0.312977  0.458484   \n",
       "423  0.614458  0.0  0.120000  0.523529  0.293233  0.387405  0.487365   \n",
       "424  0.084337  0.0  0.020741  0.088235  0.639098  0.299618  0.743682   \n",
       "425  0.433735  0.0  0.097778  0.511765  0.240602  0.286260  0.469314   \n",
       "426  0.421687  1.0  0.081481  0.276471  0.000000  0.311069  0.389892   \n",
       "427  0.698795  0.0  0.081481  0.347059  0.586466  0.282443  0.667870   \n",
       "428  0.771084  0.0  0.081481  0.335294  0.210526  0.000000  0.476534   \n",
       "429  0.096386  1.0  0.037037  0.105882  0.165414  0.238550  0.454874   \n",
       "430  0.132530  0.0  0.048889  0.135294  0.511278  0.276718  0.465704   \n",
       "431  0.566265  0.0  0.090370  0.294118  0.180451  0.276718  0.537906   \n",
       "432  0.132530  0.0  0.051852  0.211765  0.248120  0.234733  0.469314   \n",
       "433  0.843373  0.0  0.091852  0.317647  0.187970  0.284351  0.209386   \n",
       "434  0.240964  0.0  0.108148  0.347059  0.248120  0.295802  0.462094   \n",
       "435  0.469880  1.0  0.087407  0.329412  0.180451  0.295802  0.487365   \n",
       "436  0.385542  1.0  0.087407  0.300000  0.165414  0.274809  0.389892   \n",
       "437  0.421687  1.0  0.074074  0.335294  0.240602  0.270992  0.574007   \n",
       "438  0.445783  0.0  0.103704  0.447059  0.248120  0.278626  0.451264   \n",
       "439  0.590361  1.0  0.093333  0.352941  0.293233  0.324427  0.545126   \n",
       "440  0.445783  0.0  0.105185  0.388235  0.248120  0.291985  0.566787   \n",
       "441  0.445783  1.0  0.081481  0.258824  0.142857  0.272901  0.512635   \n",
       "442  0.783133  1.0  0.081481  0.258824  0.225564  0.272901  0.472924   \n",
       "443  0.493976  1.0  0.072593  0.405882  0.248120  0.299618  0.548736   \n",
       "444  0.349398  0.0  0.090370  0.335294  0.195489  0.272901  0.335740   \n",
       "445  0.542169  0.0  0.103704  0.405882  0.270677  0.255725  0.519856   \n",
       "446  0.240964  1.0  0.077037  0.300000  0.195489  0.288168  0.472924   \n",
       "447  0.638554  1.0  0.081481  0.376471  0.187970  0.379771  0.541516   \n",
       "448  0.445783  0.0  0.125926  0.464706  0.338346  0.261450  0.465704   \n",
       "449  0.433735  0.0  0.090370  0.364706  0.398496  0.335878  0.480144   \n",
       "450  0.385542  1.0  0.074074  0.288235  0.285714  0.202290  0.555957   \n",
       "451  0.939759  1.0  0.081481  0.376471  0.180451  0.242366  0.476534   \n",
       "\n",
       "          7         8         9      ...          268       269       270  \\\n",
       "1    0.150183  0.190244  0.577713    ...     0.558747  0.614458  1.000000   \n",
       "2    0.282051  0.497561  0.785924    ...     0.583812  0.783133  1.000000   \n",
       "3    0.260073  0.697561  0.586510    ...     0.687206  0.686747  1.000000   \n",
       "4    0.252747  0.502439  0.457478    ...     0.587467  0.626506  1.000000   \n",
       "5    0.241758  0.443902  0.818182    ...     0.588512  0.674699  0.853659   \n",
       "6    0.091575  0.375610  0.730205    ...     0.430809  0.626506  1.000000   \n",
       "7    0.179487  0.341463  0.700880    ...     0.441253  0.638554  1.000000   \n",
       "8    0.190476  0.307317  0.683284    ...     0.585379  0.686747  1.000000   \n",
       "9    0.175824  0.356098  0.753666    ...     0.486684  0.614458  0.853659   \n",
       "10   0.175824  0.404878  0.715543    ...     0.351436  0.626506  0.878049   \n",
       "11   0.153846  0.317073  0.539589    ...     0.361880  0.686747  1.000000   \n",
       "12   0.201465  0.395122  0.434018    ...     0.374935  0.650602  1.000000   \n",
       "13   0.179487  0.507317  0.703812    ...     0.749347  0.602410  0.780488   \n",
       "14   0.201465  0.458537  0.639296    ...     0.574413  0.686747  0.853659   \n",
       "15   0.223443  0.317073  0.609971    ...     0.548825  0.674699  1.000000   \n",
       "16   0.223443  0.297561  0.730205    ...     0.578068  0.626506  1.000000   \n",
       "17   0.051282  0.253659  0.671554    ...     0.434465  0.602410  1.000000   \n",
       "18   0.245421  0.404878  0.718475    ...     0.467363  0.614458  1.000000   \n",
       "19   0.183150  0.385366  0.469208    ...     0.559791  0.638554  1.000000   \n",
       "20   0.296703  0.892683  0.651026    ...     0.669974  0.674699  0.853659   \n",
       "21   0.252747  0.380488  0.741935    ...     0.445953  0.626506  0.853659   \n",
       "22   0.223443  0.400000  0.686217    ...     0.529504  0.662651  1.000000   \n",
       "23   0.128205  0.448780  0.516129    ...     0.480418  0.759036  1.000000   \n",
       "24   0.150183  0.400000  0.656891    ...     0.633943  0.602410  1.000000   \n",
       "25   0.318681  0.292683  0.404692    ...     0.448042  0.590361  1.000000   \n",
       "26   0.424908  0.609756  0.768328    ...     0.844909  0.638554  1.000000   \n",
       "27   0.219780  0.404878  0.533724    ...     0.631854  0.674699  1.000000   \n",
       "28   1.000000  0.482927  0.480938    ...     0.312794  0.626506  1.000000   \n",
       "29   0.706960  0.400000  0.395894    ...     0.246997  0.590361  1.000000   \n",
       "30   0.234432  0.443902  0.351906    ...     0.493473  0.686747  1.000000   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "422  0.168498  0.336585  0.692082    ...     0.631854  0.626506  1.000000   \n",
       "423  0.230769  0.517073  0.483871    ...     0.427676  0.650602  1.000000   \n",
       "424  0.432234  0.395122  0.387097    ...     0.761880  0.771084  1.000000   \n",
       "425  0.252747  0.468293  0.633431    ...     0.647520  0.698795  1.000000   \n",
       "426  0.197802  0.497561  0.621701    ...     0.643342  0.674699  1.000000   \n",
       "427  0.556777  0.448780  0.041056    ...     0.345692  0.686747  0.902439   \n",
       "428  0.043956  0.439024  0.589443    ...     0.451697  0.674699  1.000000   \n",
       "429  0.186813  0.341463  0.759531    ...     0.571802  0.638554  1.000000   \n",
       "430  0.413919  0.390244  0.832845    ...     0.488773  0.638554  0.170732   \n",
       "431  0.238095  0.492683  0.656891    ...     0.551958  0.602410  1.000000   \n",
       "432  0.439560  0.395122  0.451613    ...     1.000000  0.686747  1.000000   \n",
       "433  0.073260  0.453659  0.307918    ...     0.248042  0.686747  1.000000   \n",
       "434  0.201465  0.346341  0.439883    ...     0.337859  0.650602  0.878049   \n",
       "435  0.164835  0.463415  0.651026    ...     0.527937  0.662651  1.000000   \n",
       "436  0.146520  0.400000  0.583578    ...     0.591123  0.734940  0.853659   \n",
       "437  0.106227  0.429268  0.697947    ...     0.515927  0.602410  1.000000   \n",
       "438  0.260073  0.351220  0.507331    ...     0.548303  0.626506  0.902439   \n",
       "439  0.161172  0.560976  0.774194    ...     0.492428  0.638554  1.000000   \n",
       "440  0.234432  0.434146  0.700880    ...     0.613055  0.554217  0.780488   \n",
       "441  0.139194  0.365854  0.703812    ...     0.724282  0.674699  1.000000   \n",
       "442  0.139194  0.409756  0.387097    ...     0.345170  0.662651  1.000000   \n",
       "443  0.087912  0.546341  0.695015    ...     0.634987  0.626506  0.902439   \n",
       "444  0.402930  0.360976  0.574780    ...     0.593734  0.686747  1.000000   \n",
       "445  0.190476  0.404878  0.771261    ...     0.425065  0.626506  1.000000   \n",
       "446  0.212454  0.390244  0.630499    ...     0.587990  0.674699  1.000000   \n",
       "447  0.168498  0.570732  0.395894    ...     0.328982  0.674699  1.000000   \n",
       "448  0.340659  0.356098  0.756598    ...     0.762924  0.614458  1.000000   \n",
       "449  0.315018  0.565854  0.255132    ...     0.000000  0.819277  1.000000   \n",
       "450  0.402930  0.307317  0.662757    ...     0.650653  0.698795  0.902439   \n",
       "451  0.109890  0.380488  0.586510    ...     0.535248  0.638554  1.000000   \n",
       "\n",
       "          271       272  273      275       276       277       278  \n",
       "1    0.360169  1.000000  0.0  0.31250  0.675000  0.485714  0.500971  \n",
       "2    0.402542  0.916084  0.0  0.34375  0.783333  0.424812  0.566990  \n",
       "3    0.516949  0.923077  0.0  0.37500  0.716667  0.592481  0.648544  \n",
       "4    0.555085  0.874126  0.0  0.21875  0.825000  0.523308  0.656311  \n",
       "5    0.516949  0.902098  0.0  0.53125  0.683333  0.433835  0.451133  \n",
       "6    0.275424  1.000000  0.0  0.37500  0.583333  0.439850  0.382524  \n",
       "7    0.347458  0.933566  0.0  0.28125  0.541667  0.451128  0.377994  \n",
       "8    0.296610  0.954545  0.0  0.43750  0.675000  0.426316  0.449838  \n",
       "9    0.457627  0.940559  0.0  0.50000  0.575000  0.483459  0.412298  \n",
       "10   0.381356  0.930070  0.0  0.50000  0.575000  0.424812  0.374757  \n",
       "11   0.186441  0.923077  0.0  0.40625  0.625000  0.369173  0.361165  \n",
       "12   0.266949  0.926573  0.0  0.50000  0.541667  0.398496  0.328155  \n",
       "13   0.521186  1.000000  0.0  0.37500  0.675000  0.546617  0.564401  \n",
       "14   0.525424  1.000000  0.0  0.34375  0.641667  0.627068  0.600000  \n",
       "15   0.326271  0.972028  0.0  0.43750  0.641667  0.461654  0.451133  \n",
       "16   0.398305  0.940559  0.0  0.43750  0.691667  0.478947  0.515858  \n",
       "17   0.279661  1.000000  0.0  0.34375  0.558333  0.460902  0.384466  \n",
       "18   0.241525  1.000000  0.0  0.37500  0.541667  0.469173  0.394822  \n",
       "19   0.326271  0.968531  0.0  0.40625  0.650000  0.521805  0.499029  \n",
       "20   0.385593  0.951049  0.0  0.43750  0.775000  0.460902  0.603883  \n",
       "21   0.351695  0.937063  0.0  0.50000  0.591667  0.420301  0.376699  \n",
       "22   0.377119  0.965035  0.0  0.40625  0.641667  0.480451  0.471845  \n",
       "23   0.283898  0.982517  0.0  0.37500  0.591667  0.470677  0.436893  \n",
       "24   0.576271  1.000000  0.0  0.40625  0.708333  0.597744  0.620712  \n",
       "25   0.292373  1.000000  0.0  0.37500  0.608333  0.487970  0.438835  \n",
       "26   0.648305  0.961538  0.0  0.43750  0.716667  0.663158  0.692557  \n",
       "27   0.538136  0.937063  0.0  0.34375  0.766667  0.523308  0.604531  \n",
       "28   0.228814  1.000000  0.0  0.37500  0.383333  0.461654  0.269256  \n",
       "29   0.309322  0.863636  0.0  0.40625  0.408333  0.359398  0.209061  \n",
       "30   0.190678  0.944056  0.0  0.46875  0.591667  0.396992  0.355340  \n",
       "..        ...       ...  ...      ...       ...       ...       ...  \n",
       "422  0.597458  0.923077  0.0  0.40625  0.750000  0.578195  0.612945  \n",
       "423  0.406780  0.877622  0.0  0.56250  0.633333  0.403008  0.401294  \n",
       "424  0.423729  0.926573  0.0  0.56250  0.958333  0.608271  1.000000  \n",
       "425  0.436441  0.972028  0.0  0.43750  0.750000  0.513534  0.592233  \n",
       "426  0.368644  0.982517  0.0  0.40625  0.691667  0.489474  0.512621  \n",
       "427  0.271186  0.877622  0.0  0.37500  0.566667  0.310526  0.291909  \n",
       "428  0.283898  0.986014  0.0  0.34375  0.533333  0.510526  0.420712  \n",
       "429  0.478814  0.926573  0.0  0.46875  0.800000  0.453383  0.568285  \n",
       "430  0.830508  0.853147  0.0  0.31250  0.650000  0.424060  0.412298  \n",
       "431  0.360169  1.000000  0.0  0.43750  0.600000  0.485714  0.437540  \n",
       "432  0.724576  0.751748  0.0  0.46875  0.958333  0.445865  0.796117  \n",
       "433  0.114407  0.811189  0.0  0.34375  0.483333  0.278947  0.196117  \n",
       "434  0.432203  1.000000  0.0  0.40625  0.533333  0.512782  0.414239  \n",
       "435  0.411017  0.975524  0.0  0.50000  0.608333  0.513534  0.467961  \n",
       "436  0.419492  0.979021  0.0  0.40625  0.700000  0.475940  0.484790  \n",
       "437  0.453390  1.000000  0.0  0.56250  0.675000  0.524812  0.529450  \n",
       "438  0.572034  0.958042  0.0  0.40625  0.550000  0.558647  0.476375  \n",
       "439  0.347458  0.975524  0.0  0.50000  0.641667  0.493985  0.467961  \n",
       "440  0.703390  0.881119  0.0  0.46875  0.650000  0.519549  0.517799  \n",
       "441  0.483051  0.968531  0.0  0.46875  0.650000  0.633835  0.609061  \n",
       "442  0.279661  0.786713  0.0  0.40625  0.541667  0.303759  0.252427  \n",
       "443  0.444915  0.912587  0.0  0.40625  0.616667  0.466165  0.440777  \n",
       "444  0.330508  0.954545  0.0  0.40625  0.691667  0.438346  0.489968  \n",
       "445  0.300847  0.916084  0.0  0.12500  0.608333  0.396241  0.363754  \n",
       "446  0.305085  0.975524  0.0  0.40625  0.691667  0.464662  0.503560  \n",
       "447  0.182203  0.825175  0.0  0.46875  0.550000  0.299248  0.246602  \n",
       "448  0.661017  0.944056  0.0  0.37500  0.700000  0.618045  0.653722  \n",
       "449  0.690678  0.000000  0.0  0.71875  0.583333  0.000000  0.034951  \n",
       "450  0.508475  0.975524  0.0  0.40625  0.700000  0.520301  0.551456  \n",
       "451  0.440678  0.937063  0.0  0.40625  0.633333  0.492481  0.462136  \n",
       "\n",
       "[450 rows x 257 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[:,0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Target variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data[279]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Strategy: We have used R square as the evaluation strategy for our models. This is because it depends on teh dataset to decide whether to use the R square or the precision recall scores. After checking all the scores coming on a logistic regression, we came to the conclusion of using the standard r square for our evaluations.\n",
    "\n",
    "Best Model:\n",
    "From the below regressions, we found the following useful metrics to decide the results.\n",
    "The order of first three best models is: Linear SVM, Random Forest, Kernelized SVM. \n",
    "This is based on the r square values of the models. Linear SVM explains 65.8% of variance. 65.5% of variance is being explained by Random Forest. Kernelized SVM explains 64.6% of variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 5, 7, 15, 55]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "knnreg = KNeighborsClassifier()\n",
    "knn_grid = {'n_neighbors':[1,2, 3,5, 7, 15, 55]}\n",
    "grid_search_knn = GridSearchCV(knnreg, knn_grid, cv = 3)\n",
    "grid_search_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'n_neighbors': 3}\n",
      "Best score 0.5972222222222222\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search_knn.best_params_))\n",
    "print('Best score {}'.format(grid_search_knn.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  6  1  1  4  1  1  1  1  1  3  1  1  1  1  1  1 10  1  3  1  1  1  1  1\n",
      "  1  1  1  3  1  1  1  1  2  1  1  6  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 10  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n",
      "KNN R-squared test score: 0.6444444444444445\n",
      "KNN R-squared score (training): 0.702777778\n"
     ]
    }
   ],
   "source": [
    "knnreg = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)\n",
    "\n",
    "print(knnreg.predict(X_test))\n",
    "print('KNN R-squared test score: {}'\n",
    "     .format(knnreg.score(X_test, y_test)))\n",
    "print('KNN R-squared score (training): {:.9f}'\n",
    "     .format(knnreg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55645161  0.63333333  0.60344828]\n",
      "[ 0.54545455  0.5862069   0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_knnreg_train = cross_val_score(knnreg, X_train, y_train, cv = 3)\n",
    "print(scores_knnreg_train)\n",
    "scores_knnreg_test = cross_val_score(knnreg, X_test, y_test, cv = 3)\n",
    "print(scores_knnreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.597744407\n",
      "Average cross-validation score for testing set: 0.567696671\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_knnreg_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_knnreg_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logr= LogisticRegression(random_state=0)\n",
    "logr_grid = {'C':[0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(logr, logr_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 1}\n",
      "Best score 0.6972222222222222\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression R-squared test score: 0.7111111111111111\n",
      "Logistic regression R-squared score (training): 0.797222222\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "print('Logistic regression R-squared test score: {}'\n",
    "     .format(lr.score(X_test, y_test)))\n",
    "print('Logistic regression R-squared score (training): {:.9f}'\n",
    "     .format(lr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier (default settings)\n",
      " [[51  0  0  0  0  0  0  1  0]\n",
      " [ 6  4  0  0  0  0  0  0  1]\n",
      " [ 0  0  3  0  0  0  0  0  0]\n",
      " [ 2  0  0  1  0  0  0  1  0]\n",
      " [ 2  0  0  0  0  0  0  2  0]\n",
      " [ 5  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  0  0]\n",
      " [ 2  0  0  0  0  0  0  4  0]\n",
      " [ 2  0  0  0  0  0  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "lr_predicted = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.73      0.98      0.84        52\n",
      "          2       1.00      0.36      0.53        11\n",
      "          3       1.00      1.00      1.00         3\n",
      "          4       1.00      0.25      0.40         4\n",
      "          5       0.00      0.00      0.00         4\n",
      "          6       0.00      0.00      0.00         6\n",
      "          7       1.00      1.00      1.00         1\n",
      "          9       0.40      0.67      0.50         6\n",
      "         10       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.66      0.71      0.64        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 9, does not match size of target_names, 15\n",
      "  .format(len(labels), len(target_names))\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Logistic regression\\n',classification_report(y_test, lr_predicted, target_names=['1', '2','3','4','5','6','7','9','10','11','12','13','14','15','16']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7016129   0.69166667  0.69827586]\n",
      "[ 0.57575758  0.65517241  0.60714286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_lr_train = cross_val_score(lr, X_train, y_train, cv = 3)\n",
    "print(scores_lr_train)\n",
    "scores_lr_test = cross_val_score(lr, X_test, y_test, cv = 3)\n",
    "print(scores_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.697185144\n",
      "Average cross-validation score for testing set: 0.612690949\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_lr_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_lr_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10,100]}\n",
    "svc = SVC(kernel='linear',random_state=0)\n",
    "grid_search = GridSearchCV(svc, param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 1}\n",
      "Best score 0.7\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC R-squared score (training): 0.86\n",
      "Linear SVC R-squared score (test): 0.70\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'linear', C=1).fit(X_train, y_train)\n",
    "print('Linear SVC R-squared score (training): {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Linear SVC R-squared score (test): {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65322581  0.71666667  0.73275862]\n",
      "[ 0.60606061  0.68965517  0.67857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_svc_train = cross_val_score(clf, X_train, y_train, cv = 3)\n",
    "print(scores_svc_train)\n",
    "scores_svc_test = cross_val_score(clf, X_test, y_test, cv = 3)\n",
    "print(scores_svc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.700883698\n",
      "Average cross-validation score for testing set: 0.658095736\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_svc_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_svc_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC-Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel = 'rbf',random_state=0)\n",
    "param_grid_kernel = {'C': [ 0.01, 0.1, 1, 10,100], \n",
    "            'gamma': [0.001,0.01, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_kernel = GridSearchCV(svc, param_grid_kernel, cv = 3)\n",
    "grid_search_kernel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 100, 'gamma': 0.01}\n",
      "Best score 0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search_kernel.best_params_))\n",
    "print('Best score {}'.format(grid_search_kernel.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernelized SVC R-squared score (training): 0.930555556\n",
      "Kernelized SVC R-squared score (test): 0.711111111\n"
     ]
    }
   ],
   "source": [
    "clf=SVC(kernel = 'rbf',C=100,gamma=.01,random_state=0)\n",
    "svc_K=clf.fit(X_train, y_train)\n",
    "print('Kernelized SVC R-squared score (training): {:.9f}'\n",
    "     .format(svc_K.score(X_train, y_train)))\n",
    "print('Kernelized SVC R-squared score (test): {:.9f}'\n",
    "     .format(svc_K.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68548387  0.71666667  0.72413793]\n",
      "[ 0.60606061  0.68965517  0.64285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_clf_train = cross_val_score(svc_K, X_train, y_train, cv = 3)\n",
    "print(scores_clf_train)\n",
    "scores_clf_test = cross_val_score(svc_K, X_test, y_test, cv = 3)\n",
    "print(scores_clf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.708762823\n",
      "Average cross-validation score for testing set: 0.646190974\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_clf_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_clf_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [1, 3, 5, 7, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "Decision_tree=DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {'max_depth': [ 1,3,5,7,10,15]}\n",
    "grid_search = GridSearchCV(Decision_tree, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'max_depth': 3}\n",
      "Best score 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.677777778\n",
      "Accuracy of Decision Tree classifier on test set: 0.677777778\n"
     ]
    }
   ],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.9f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.9f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.58064516  0.59166667  0.68965517]\n",
      "[ 0.57575758  0.48275862  0.64285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_clf2_train = cross_val_score(clf2, X_train, y_train, cv = 3)\n",
    "print(scores_clf2_train)\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv = 3)\n",
    "print(scores_clf2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.620655667\n",
      "Average cross-validation score for testing set: 0.567124446\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_clf2_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_clf2_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles of Decision Trees: Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf3 = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [1, 3, 5, 8, 10, 15, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_features': [ 1,3,5,8,10,15,20,30]}\n",
    "grid_search = GridSearchCV(clf3, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'max_features': 30}\n",
      "Best score 0.6777777777777778\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  2  2  1  1  1  2  1  3  1  1  1  1  1  1 10  1  3  1  1  2  1  2\n",
      "  1 10 10  3  5  1 10  1  1  1  2  1 10  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  2  1  1 10  1 10 10  1  2  1  9  1  1  1  1  1  1  2  1  1 10\n",
      "  1  1  1  2  1 10  1  1  1  1  1  1  6  1  1]\n",
      "Random Forest R-squared test score: 0.7\n",
      "Random Forest R-squared score (training): 0.983333333\n"
     ]
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier(max_features = 10, random_state = 0)\n",
    "Random_forest=clf3.fit(X_train, y_train)\n",
    "\n",
    "print(Random_forest.predict(X_test))\n",
    "print('Random Forest R-squared test score: {}'\n",
    "     .format(Random_forest.score(X_test, y_test)))\n",
    "print('Random Forest R-squared score (training): {:.9f}'\n",
    "     .format(Random_forest.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63709677  0.675       0.64655172]\n",
      "[ 0.66666667  0.62068966  0.67857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_Random_forest_train = cross_val_score(Random_forest, X_train, y_train, cv = 3)\n",
    "print(scores_Random_forest_train)\n",
    "scores_Random_forest_test = cross_val_score(Random_forest, X_test, y_test, cv = 3)\n",
    "print(scores_Random_forest_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.652882833\n",
      "Average cross-validation score for testing set: 0.655309250\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_Random_forest_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_Random_forest_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf4 = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.01, 0.1, 1, 10, 20], 'max_depth': [1, 2, 3, 5, 8, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'learning_rate': [ 0.01,0.1,1,10,20],'max_depth': [ 1,2,3,5,8,10,15]}\n",
    "grid_search = GridSearchCV(clf4, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'learning_rate': 0.01, 'max_depth': 1}\n",
      "Best score 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBDT classifier on training set: 0.94\n",
      "Accuracy of GBDT classifier on test set: 0.71\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 1, random_state = 0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of GBDT classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of GBDT classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62903226  0.74166667  0.71551724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6969697   0.65517241  0.75      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_GBDT_train = cross_val_score(clf, X_train, y_train, cv = 3)\n",
    "print(scores_GBDT_train)\n",
    "scores_GBDT_test = cross_val_score(clf, X_test, y_test, cv = 3)\n",
    "print(scores_GBDT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.695405389\n",
      "Average cross-validation score for testing set: 0.700714037\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_GBDT_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_GBDT_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "The following code creates and train a voting classifier in Scikit Learn composed of following diverse classifiers\n",
    "1. KNN classifcation\n",
    "2. Logistic Regression\n",
    "3. Support Vector Machine\n",
    "4. Decision Tree\n",
    "5. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1,...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "knn_clf=KNeighborsClassifier()\n",
    "log_clf = LogisticRegression(random_state=0)\n",
    "svm_clf = SVC(random_state=0)\n",
    "dt_clf=DecisionTreeClassifier(random_state=0)\n",
    "rnd_clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('knn', knn_clf),\n",
    "                                          ('lr', log_clf),\n",
    "                                          ('svm', svm_clf),\n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('dt', dt_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1,...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(random_state=0)\n",
    "rnd_clf = RandomForestClassifier(random_state=0)\n",
    "svm_clf = SVC(probability=True, random_state=0)\n",
    "dt_clf=DecisionTreeClassifier(random_state=0)\n",
    "knn_clf=KNeighborsClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('knn', knn_clf),\n",
    "                                          ('lr', log_clf),\n",
    "                                          ('svm', svm_clf),\n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('dt', dt_clf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.622222222222\n",
      "LogisticRegression 0.711111111111\n",
      "RandomForestClassifier 0.7\n",
      "SVC 0.577777777778\n",
      "DecisionTreeClassifier 0.633333333333\n",
      "VotingClassifier 0.722222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (knn_clf, log_clf, rnd_clf, svm_clf, dt_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemtation of different bagging and boosting techniques.\n",
    "Following results have been interpreted from the below regressions:\n",
    "\n",
    "Bagging: Well, there has been significant change in the performance of the model if we have used bagging instead of no bagging.\n",
    "    The first block of code shows use of bagging classifier, while the second block shows the results without the bagging classifiers. \n",
    "    The order of best 3 models after using bagging are: Random Forest, decision trees and Logistic Regression. \n",
    "    \n",
    "    implementing random forest gave r square result of 73.3%, which is the highest percentage of variance explained by any model in our project. The main reason for this could be that random forest is an ensemble approach to the decision trees and performs bagging on all the data each time.\n",
    "    \n",
    "    Decision tree and logistic regression explain 72% of variance in the data. \n",
    "    \n",
    "    Also, We see no difference in the accuracy of the model if we use bagging in logistic regression because logistic regression is already a low variance estimator.\n",
    "    \n",
    " Boosting: Adaptive boosting has been applied on all the models except KNN. Gradient boosting has limited applicabiltiy.\n",
    " Results: The order of models which show best results are: decision tree, linear SVM and random forrest. The boosting models in general do not show good results in terms of r square values among regular method, bagging and PCA. The highest r square we could get is 71% from decision tree.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=0), \n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633333333333\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733333333333\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=0),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711111111111\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred == y_pred_rf) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(KNeighborsClassifier(n_neighbors = 3),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644444444444\n"
     ]
    }
   ],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors = 3, n_jobs=-1)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see no difference in the accuracy of the model if we use bagging in logistic regression because logistic regression is already a low variance estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711111111111\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(LogisticRegression(random_state=0),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711111111111\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(random_state=0)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711111111111\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(SVC(kernel='linear',random_state=0),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. SVC- Kernel\n",
    "\n",
    "There is no effect of bagging on kernalized SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577777777778\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(SVC(kernel = 'rbf',random_state=0),\n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577777777778\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf',random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7055555555555556"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=0), n_estimators=500,\n",
    "                             bootstrap=True,n_jobs=-1, oob_score=True, random_state=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73333333333333328"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=200,\n",
    "                             algorithm=\"SAMME\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644444444444\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(RandomForestClassifier(max_features = 10, random_state = 0), n_estimators=200,\n",
    "                             algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655555555556\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(LogisticRegression(C=1, random_state=0), n_estimators=200,\n",
    "                             algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577777777778\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(SVC(kernel='linear', C=1,random_state=0), n_estimators=200,\n",
    "                             algorithm=\"SAMME\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. SVM-Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577777777778\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(SVC(kernel = 'rbf',C=100,gamma=.01,random_state=0), n_estimators=200,\n",
    "                             algorithm=\"SAMME\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=1.0, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=3, presort='auto',\n",
       "              random_state=0, subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GradientBoosting classifier on training set: 0.091666667\n",
      "Accuracy of GradientBoosting classifier on test set: 0.122222222\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of GradientBoosting classifier on training set: {:.9f}'\n",
    "     .format(gbrt.score(X_train, y_train)))\n",
    "print('Accuracy of GradientBoosting classifier on test set: {:.9f}'\n",
    "     .format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrt_grid = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [1, 2, 3, 5, 8, 10], 'n_estimators': [1, 2, 3, 5, 8, 10], 'learning_rate': [0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_depth': [ 1,2,3,5,8,10],'n_estimators':[1,2,3,5,8,10],'learning_rate':[0.01,0.1,1,10]}\n",
    "grid_search = GridSearchCV(gbrt_grid, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingClassifier(max_depth=1, n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "gbrt_best.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost classifier on training set: 0.744444444\n",
      "Accuracy of AdaBoost classifier on test set: 0.788888889\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of AdaBoost classifier on training set: {:.9f}'\n",
    "     .format(gbrt_best.score(X_train, y_train)))\n",
    "print('Accuracy of AdaBoost classifier on test set: {:.9f}'\n",
    "     .format(gbrt_best.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_slow = GradientBoostingClassifier(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=0)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost classifier on training set: 1.000000000\n",
      "Accuracy of AdaBoost classifier on test set: 1.000000000\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of AdaBoost classifier on training set: {:.9f}'\n",
    "     .format(gbrt_slow.score(X_train, y_train)))\n",
    "print('Accuracy of AdaBoost classifier on test set: {:.9f}'\n",
    "     .format(gbrt_slow.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the 3D points projected on the plane (PCA 2D subspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3D_inv = pca.inverse_transform(X2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0773156705730376"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(np.square(X3D_inv - X), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.97511808e-02,  -6.12411694e-02,   2.16945171e-03,\n",
       "          1.28833960e-02,   2.04167428e-02,  -1.53924894e-03,\n",
       "          4.29224954e-03,   2.45989033e-02,   1.58684575e-02,\n",
       "         -1.47211540e-01,   2.08248065e-02,   3.57916388e-02,\n",
       "         -5.34034646e-02,  -8.64249796e-03,   2.77052508e-02,\n",
       "         -2.04342109e-03,   2.65535172e-03,   1.89314278e-02,\n",
       "          7.41905121e-03,   1.10929997e-02,  -3.67769925e-04,\n",
       "         -7.25215124e-02,  -7.63293624e-02,   2.14892199e-01,\n",
       "          1.15491668e-02,   1.96690872e-03,  -7.03410859e-02,\n",
       "          1.50731610e-02,  -2.87614080e-02,  -8.88058868e-03,\n",
       "          4.96825141e-04,  -1.60442563e-03,   1.74423911e-03,\n",
       "          4.79715040e-04,  -1.85415454e-01,   1.62663022e-01,\n",
       "          1.21331453e-02,   4.30696860e-03,  -1.52632398e-01,\n",
       "         -2.61681547e-03,  -2.00840991e-02,  -3.04355364e-03,\n",
       "         -5.34303600e-03,  -2.63019642e-03,   2.01633255e-03,\n",
       "          2.45441034e-02,   1.26012642e-01,  -9.42207722e-02,\n",
       "         -8.44927286e-03,  -3.53610488e-03,   1.42904222e-01,\n",
       "          1.44663685e-02,   4.83505124e-04,  -1.38065122e-02,\n",
       "         -3.38672602e-03,   5.84599423e-04,  -3.66167452e-03,\n",
       "          1.04144958e-02,   1.25810881e-01,  -1.86157562e-01,\n",
       "         -6.70820532e-03,   1.12470682e-01,   1.70164929e-02,\n",
       "          2.94560481e-03,   4.90064557e-03,  -5.25967451e-03,\n",
       "          1.24599574e-03,  -2.59512408e-02,  -1.41221284e-01,\n",
       "          1.80135946e-01,   7.77297620e-03,   8.63300555e-03,\n",
       "         -1.23319340e-01,   6.41148098e-03,  -7.83369864e-03,\n",
       "         -4.92592356e-03,   3.82703528e-03,   1.61730984e-03,\n",
       "          9.57832336e-02,  -2.74258770e-02,  -9.89589050e-02,\n",
       "          3.72535690e-03,   8.59433601e-03,  -1.92781016e-02,\n",
       "          3.25950927e-03,   2.80034007e-03,  -1.94405319e-02,\n",
       "         -1.30563913e-02,  -1.35734698e-02,   7.81703880e-04,\n",
       "          7.33626969e-02,  -2.17049907e-02,  -6.26750671e-02,\n",
       "          3.49101130e-03,   8.24122604e-03,  -1.64941159e-02,\n",
       "          1.12780370e-02,   7.35951990e-03,  -1.75367020e-02,\n",
       "         -2.15697208e-02,   1.02822733e-02,  -4.68067953e-03,\n",
       "          7.92321865e-02,  -5.10254462e-02,  -2.10465942e-03,\n",
       "         -9.89592029e-04,  -2.84288798e-04,  -4.74223219e-02,\n",
       "          3.16074963e-02,  -3.53858768e-03,  -3.11490308e-03,\n",
       "         -1.37822426e-02,   9.62359482e-03,   1.69355670e-02,\n",
       "          8.48376351e-03,  -4.92974506e-02,   6.30952474e-02,\n",
       "         -2.71861745e-02,  -1.84770820e-03,  -5.65239509e-02,\n",
       "          1.11625389e-02,  -3.62826834e-03,  -2.41491083e-04,\n",
       "          1.40658180e-02,  -3.53567466e-02,  -6.32458395e-03,\n",
       "          7.55140414e-02,   1.51580039e-03,  -1.53940874e-02,\n",
       "          9.49039138e-03,   3.54948614e-03,  -2.10475921e-03,\n",
       "         -6.02984892e-02,  -1.08569541e-02,   1.17302496e-01,\n",
       "          5.25683191e-03,  -2.99430235e-02,  -3.04204999e-03,\n",
       "         -4.21040542e-04,   1.13546616e-03,  -3.32302726e-03,\n",
       "         -1.98412354e-02,  -2.95457474e-02,   4.43441414e-02,\n",
       "          2.01179248e-02,  -4.61318728e-03,   1.67482525e-02,\n",
       "         -3.44001265e-02,   2.65186423e-02,   6.57592557e-03,\n",
       "          1.71866283e-02,   7.72843800e-02,  -2.09522819e-01,\n",
       "         -9.88854597e-02,   4.30976877e-03,  -2.69670623e-03,\n",
       "         -1.74317569e-02,  -2.87755042e-02,  -1.64305066e-01,\n",
       "         -1.61229053e-01,   4.44376143e-02,  -1.43042088e-02,\n",
       "         -1.67970550e-01,  -1.58072178e-01,   3.44399547e-03,\n",
       "         -4.76905505e-03,  -2.77211830e-02,   5.01507083e-03,\n",
       "         -1.52273237e-01,  -1.29632104e-01,   6.02281188e-03,\n",
       "          2.36124040e-02,   4.69637042e-02,   9.27708753e-02,\n",
       "         -1.02709616e-02,   3.53610488e-03,   6.60108317e-03,\n",
       "          3.81575990e-02,   5.18529787e-02,   1.05926055e-01,\n",
       "         -2.44108876e-02,  -1.82565416e-03,   1.51073201e-01,\n",
       "          1.10733018e-01,  -4.90357627e-03,   2.92356007e-02,\n",
       "         -2.26678253e-02,   8.46804805e-02,   1.13857661e-01,\n",
       "          4.19727823e-02,   1.07805340e-02,  -2.07782344e-01,\n",
       "         -1.25976139e-01,   2.97292518e-03,  -8.22903576e-03,\n",
       "         -2.16250183e-02,  -1.09762451e-02,  -1.83289898e-01,\n",
       "         -1.71709338e-01,   7.55440641e-03,  -7.25014772e-02,\n",
       "         -2.24004623e-02,   6.05673374e-02,   5.63441891e-03,\n",
       "         -9.48449527e-03,   9.89656515e-03,   1.15829062e-02,\n",
       "         -1.25567302e-03,   8.67437734e-03,  -1.35508415e-03,\n",
       "         -3.83507072e-02,  -4.61471955e-02,   7.96418172e-02,\n",
       "          3.26179569e-03,  -4.84830259e-03,   7.83553377e-03,\n",
       "         -2.20114544e-02,   2.90986011e-03,  -1.46370156e-02,\n",
       "         -3.68659561e-04,  -3.72795802e-02,  -7.04938104e-02,\n",
       "          2.66995838e-02,   4.38921276e-04,  -7.52487878e-04,\n",
       "         -8.62913551e-03,  -2.45618177e-02,  -2.39067076e-02,\n",
       "         -5.82511342e-02,   1.79005097e-02,  -1.04123871e-02,\n",
       "         -1.15295135e-01,  -2.98040557e-02,  -1.63413597e-02,\n",
       "          1.87552300e-03,  -1.29220362e-02,  -2.54470314e-02,\n",
       "         -8.23279660e-02,  -6.89889577e-02,   1.51737323e-02,\n",
       "          5.99979414e-03,  -1.18691284e-01,  -5.17538764e-02,\n",
       "          1.28493567e-03,  -5.81522132e-03,  -3.84838181e-02,\n",
       "         -9.98705853e-02,  -9.45550170e-02,   5.99437467e-03,\n",
       "          6.01348077e-02,  -1.11229327e-01,  -4.12416069e-02,\n",
       "          4.98676684e-03,  -5.77472813e-03,  -3.45585794e-02,\n",
       "         -7.36624918e-02,  -8.76633364e-02],\n",
       "       [ -4.86012505e-02,  -6.13330806e-01,   1.12249569e-02,\n",
       "          1.13578402e-02,   7.49574668e-02,   6.68003445e-03,\n",
       "         -2.60653750e-02,   2.64359515e-02,   1.93142601e-02,\n",
       "          2.21899733e-03,   5.39567281e-02,  -6.65107910e-02,\n",
       "          1.90526103e-01,   1.18070372e-03,  -1.28468961e-03,\n",
       "         -5.19441823e-03,  -1.35634366e-02,   6.51202291e-03,\n",
       "         -2.95850749e-03,  -1.41377345e-03,   4.34116993e-03,\n",
       "          9.77767115e-02,  -6.09003658e-02,   1.14395277e-01,\n",
       "          9.47362761e-03,   8.22233693e-03,   3.84061686e-02,\n",
       "          5.71258666e-04,   3.54771948e-02,   5.67808643e-03,\n",
       "         -5.58254487e-03,  -1.33662453e-03,  -2.01093665e-03,\n",
       "          3.34284503e-02,   3.50214420e-02,   2.72577828e-03,\n",
       "          2.85198376e-02,   9.90821863e-03,   7.25521632e-02,\n",
       "          2.43916435e-03,   1.44786850e-02,  -8.41264631e-05,\n",
       "         -6.03531972e-03,   1.00177028e-02,   2.96311391e-03,\n",
       "         -1.84775528e-01,   1.96597798e-01,   1.79955467e-01,\n",
       "          9.83202435e-02,   6.20023428e-03,   2.65457052e-01,\n",
       "         -6.94517608e-03,   1.30315699e-02,   9.05545686e-03,\n",
       "         -3.47095750e-03,   9.86202911e-03,  -1.54561252e-03,\n",
       "          1.13699517e-03,  -3.39775641e-02,   1.60350817e-01,\n",
       "          2.17400224e-02,   7.65222570e-04,   1.37286644e-03,\n",
       "         -4.39192596e-04,   4.84266308e-03,   1.16136186e-02,\n",
       "          1.54042474e-03,   9.85890568e-02,  -2.56770320e-02,\n",
       "          3.82646576e-02,   7.69721535e-03,   1.33518280e-02,\n",
       "          5.46688197e-02,  -1.49381179e-03,   1.52447564e-02,\n",
       "          3.37737044e-03,  -1.16973078e-03,  -5.52351905e-03,\n",
       "         -4.40252637e-02,   4.40335645e-02,   7.97055862e-03,\n",
       "          8.28909627e-02,   9.96253809e-03,   1.04235674e-01,\n",
       "          1.09776565e-02,   4.56356471e-02,   2.30290830e-03,\n",
       "          2.61818910e-02,  -1.43596177e-03,  -2.17762272e-03,\n",
       "          4.10488815e-04,   3.09193049e-02,  -1.05305279e-02,\n",
       "          3.54652541e-02,   1.80489269e-02,   6.53440531e-02,\n",
       "          8.06901738e-03,   1.97934266e-02,   2.54909796e-02,\n",
       "          2.54843785e-02,   1.17818409e-02,   1.91125411e-02,\n",
       "          4.03378003e-02,   2.05769071e-03,   1.28022923e-02,\n",
       "          1.64937883e-02,   1.40115032e-02,   1.77297032e-02,\n",
       "          1.68675508e-02,   1.31517103e-02,   5.41474086e-04,\n",
       "          2.83540048e-02,   3.72455455e-03,   4.62856310e-03,\n",
       "          4.81131624e-02,  -2.58426747e-02,   3.56893071e-02,\n",
       "          9.58151022e-03,   2.51147456e-03,   2.14552432e-02,\n",
       "          2.83063505e-03,   5.86332674e-03,   8.66266366e-03,\n",
       "          2.64114013e-02,   8.58202837e-02,  -4.48617452e-02,\n",
       "          6.68497601e-02,  -6.57327248e-03,   4.80161123e-03,\n",
       "         -9.23075352e-03,  -2.10523011e-04,   9.56230946e-03,\n",
       "          9.99088692e-02,  -5.49582426e-02,   1.19142374e-01,\n",
       "         -3.98262746e-03,   2.36702638e-02,  -2.69691036e-03,\n",
       "          5.87050117e-03,   2.20999901e-03,  -8.97224826e-03,\n",
       "          1.51294479e-02,  -6.50473357e-02,  -3.14633701e-02,\n",
       "         -7.95523238e-02,   1.59604297e-03,   8.29577855e-03,\n",
       "         -2.05691168e-03,  -4.22992206e-02,  -6.39161413e-02,\n",
       "          3.00547878e-02,  -1.20536467e-01,  -4.26843257e-03,\n",
       "         -5.37773397e-02,   5.29217071e-03,  -9.12458571e-03,\n",
       "          1.44029661e-02,   1.02649864e-02,  -5.81961482e-02,\n",
       "         -3.47498198e-02,   1.38846628e-03,  -2.47609467e-02,\n",
       "          5.78140133e-02,  -1.54345938e-02,   7.69089987e-03,\n",
       "         -9.51512750e-03,   1.07477159e-02,   9.19926142e-03,\n",
       "          1.85175511e-02,   2.34916622e-02,  -1.87063503e-02,\n",
       "          1.66516019e-01,   7.14118878e-02,  -1.66148284e-01,\n",
       "          9.91858868e-02,  -6.20023428e-03,  -1.71210574e-02,\n",
       "         -5.92345418e-03,   6.12118937e-02,   6.45455249e-02,\n",
       "          1.10787089e-02,  -6.02132712e-03,   7.88064201e-04,\n",
       "         -9.10410874e-02,   1.85191649e-02,  -5.43741746e-03,\n",
       "         -4.26956969e-03,  -3.13227833e-02,  -5.12935743e-02,\n",
       "          6.70910405e-03,  -7.25913383e-02,   3.00973096e-02,\n",
       "         -2.83777506e-02,   2.94142843e-03,  -1.28474679e-02,\n",
       "          1.79866343e-02,   5.70189749e-03,  -1.82764592e-02,\n",
       "         -1.07603031e-02,  -8.80107108e-03,   4.97189496e-02,\n",
       "          4.94292502e-02,  -3.92759444e-03,   3.81619406e-02,\n",
       "         -8.51489769e-03,  -2.45552036e-02,  -2.11898403e-05,\n",
       "          2.29652868e-02,   2.91480615e-02,  -8.19167769e-03,\n",
       "          6.77969117e-03,   6.52525076e-02,  -2.58436551e-02,\n",
       "          1.86525217e-02,  -1.37024644e-02,  -1.35487506e-02,\n",
       "         -1.33530767e-02,   1.58854464e-02,   8.80383757e-03,\n",
       "          1.12518563e-02,  -2.06505795e-02,   9.84353319e-02,\n",
       "         -7.08808583e-02,   1.28293415e-02,  -9.48597450e-03,\n",
       "          4.99257443e-03,   5.24313062e-03,  -1.06608087e-02,\n",
       "         -8.77802276e-03,   1.29647603e-02,  -2.00751108e-02,\n",
       "          8.16958691e-02,  -6.94053551e-02,   9.74067903e-03,\n",
       "         -2.32102386e-03,   1.67219356e-02,   1.11341595e-02,\n",
       "         -1.80234156e-02,  -4.07584176e-03,   1.72475210e-02,\n",
       "         -3.98729188e-02,   5.45317286e-02,  -5.21094913e-02,\n",
       "         -5.80260410e-03,   1.60661351e-02,   5.45573371e-03,\n",
       "         -4.03903894e-02,  -2.28224417e-02,   1.87791556e-02,\n",
       "         -1.09757432e-01,   4.17289327e-03,  -3.61078909e-02,\n",
       "         -3.85873984e-03,   1.44553738e-02,  -3.53437925e-03,\n",
       "         -5.15191219e-02,  -4.11833713e-02]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance ratio:\n",
    "The first dimension explains 11.19% of the variance, while the second explains 9.93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11194233,  0.09399226])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting down to 2D, we lost about 79.4% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79406541156399379"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95127441954013714"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 83)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_reduced = pca.fit_transform(X_test)\n",
    "X_recovered = pca.inverse_transform(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 83)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average cross-validation score for testing and training set is lamost same when we use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 5, 7, 15, 55]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=X_reduced\n",
    "X_test = X_test_reduced\n",
    "knn_grid = {'n_neighbors':[1,2, 3,5, 7, 15, 55]}\n",
    "grid_search_knn = GridSearchCV(knnreg, knn_grid, cv = 3)\n",
    "grid_search_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'n_neighbors': 3}\n",
      "Best score 0.6055555555555555\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search_knn.best_params_))\n",
    "print('Best score {}'.format(grid_search_knn.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  1  1  1  1  1  1  1  1  1 10  1  1  1  1  1  1  1  1  1  2  1  6  5  6\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
      "  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  2  5 10  1  1  1  1  1  2  1  1]\n",
      "KNN R-squared test score: 0.5333333333333333\n",
      "KNN R-squared score (training): 0.702777778\n"
     ]
    }
   ],
   "source": [
    "knnreg = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)\n",
    "\n",
    "print(knnreg.predict(X_test))\n",
    "print('KNN R-squared test score: {}'\n",
    "     .format(knnreg.score(X_test, y_test)))\n",
    "print('KNN R-squared score (training): {:.9f}'\n",
    "     .format(knnreg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.57258065  0.63333333  0.61206897]\n",
      "[ 0.54545455  0.5862069   0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_knnreg_train = cross_val_score(knnreg, X_train, y_train, cv = 3)\n",
    "print(scores_knnreg_train)\n",
    "scores_knnreg_test = cross_val_score(knnreg, X_test, y_test, cv = 3)\n",
    "print(scores_knnreg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.605994315\n",
      "Average cross-validation score for testing set: 0.567696671\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_knnreg_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_knnreg_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average cross-validation score for testing and training set is little more when we use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr= LogisticRegression(random_state=0)\n",
    "logr_grid = {'C':[0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(logr, logr_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 10}\n",
      "Best score 0.7055555555555556\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression R-squared test score: 0.5111111111111111\n",
      "Logistic regression R-squared score (training): 0.875000000\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10).fit(X_train, y_train)\n",
    "print('Logistic regression R-squared test score: {}'\n",
    "     .format(lr.score(X_test, y_test)))\n",
    "print('Logistic regression R-squared score (training): {:.9f}'\n",
    "     .format(lr.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression classifier (default settings)\n",
      " [[45  2  0  1  0  1  0  0  3  0]\n",
      " [ 8  0  0  1  0  1  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0]\n",
      " [ 3  1  0  0  0  0  0  0  0  0]\n",
      " [ 3  0  1  0  0  0  0  0  0  0]\n",
      " [ 4  0  0  0  0  0  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0]\n",
      " [ 5  0  0  0  0  0  0  0  1  0]\n",
      " [ 1  1  0  0  0  0  0  0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "lr_predicted = lr.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.67741935  0.725       0.71551724]\n",
      "[ 0.63636364  0.72413793  0.71428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_lr_train = cross_val_score(lr, X_train, y_train, cv = 3)\n",
    "print(scores_lr_train)\n",
    "scores_lr_test = cross_val_score(lr, X_test, y_test, cv = 3)\n",
    "print(scores_lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.705978865\n",
      "Average cross-validation score for testing set: 0.691595761\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_lr_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_lr_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average cross-validation score for testing and training set is little more when we use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10,100]}\n",
    "svc = SVC(kernel='linear',random_state=0)\n",
    "grid_search = GridSearchCV(svc, param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 1}\n",
      "Best score 0.7027777777777777\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC R-squared score (training): 0.85\n",
      "Linear SVC R-squared score (test): 0.51\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'linear', C=1).fit(X_train, y_train)\n",
    "print('Linear SVC R-squared score (training): {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Linear SVC R-squared score (test): {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66129032  0.70833333  0.74137931]\n",
      "[ 0.60606061  0.68965517  0.67857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_svc_train = cross_val_score(clf, X_train, y_train, cv = 3)\n",
    "print(scores_svc_train)\n",
    "scores_svc_test = cross_val_score(clf, X_test, y_test, cv = 3)\n",
    "print(scores_svc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.703667655\n",
      "Average cross-validation score for testing set: 0.658095736\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_svc_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_svc_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC-Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average cross-validation score for testing and training set is same for SVC-Kernel with and without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel = 'rbf',random_state=0)\n",
    "param_grid_kernel = {'C': [ 0.01, 0.1, 1, 10,100], \n",
    "            'gamma': [0.001,0.01, 0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_kernel = GridSearchCV(svc, param_grid_kernel, cv = 3)\n",
    "grid_search_kernel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'C': 100, 'gamma': 0.01}\n",
      "Best score 0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search_kernel.best_params_))\n",
    "print('Best score {}'.format(grid_search_kernel.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernelized SVC R-squared score (training): 0.897222222\n",
      "Kernelized SVC R-squared score (test): 0.500000000\n"
     ]
    }
   ],
   "source": [
    "clf=SVC(kernel = 'rbf',C=100,gamma=.01,random_state=0)\n",
    "svc_K=clf.fit(X_train, y_train)\n",
    "print('Kernelized SVC R-squared score (training): {:.9f}'\n",
    "     .format(svc_K.score(X_train, y_train)))\n",
    "print('Kernelized SVC R-squared score (test): {:.9f}'\n",
    "     .format(svc_K.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69354839  0.68333333  0.75      ]\n",
      "[ 0.60606061  0.68965517  0.64285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_clf_train = cross_val_score(svc_K, X_train, y_train, cv = 3)\n",
    "print(scores_clf_train)\n",
    "scores_clf_test = cross_val_score(svc_K, X_test, y_test, cv = 3)\n",
    "print(scores_clf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.708960573\n",
      "Average cross-validation score for testing set: 0.646190974\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_clf_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_clf_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [1, 3, 5, 7, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_tree=DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {'max_depth': [ 1,3,5,7,10,15]}\n",
    "grid_search = GridSearchCV(Decision_tree, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'max_depth': 1}\n",
      "Best score 0.5444444444444444\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.627777778\n",
      "Accuracy of Decision Tree classifier on test set: 0.566666667\n"
     ]
    }
   ],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.9f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.9f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55645161  0.525       0.54310345]\n",
      "[ 0.48484848  0.48275862  0.42857143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores_clf2_train = cross_val_score(clf2, X_train, y_train, cv = 3)\n",
    "print(scores_clf2_train)\n",
    "scores_clf2_test = cross_val_score(clf2, X_test, y_test, cv = 3)\n",
    "print(scores_clf2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.541518354\n",
      "Average cross-validation score for testing set: 0.465392845\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_clf2_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_clf2_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles of Decision Trees: Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf3 = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [1, 3, 5, 8, 10, 15, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'max_features': [ 1,3,5,8,10,15,20,30]}\n",
    "grid_search = GridSearchCV(clf3, param_grid, cv = 3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'max_features': 10}\n",
      "Best score 0.575\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter {}'.format(grid_search.best_params_))\n",
    "print('Best score {}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  3  1  1\n",
      "  1  1  1  1  1  1  1  1  1 10  1  1  1  1  1  1  1  1  1  1  1 10  1  1  1\n",
      "  1  1  1  1  4  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
      "  4  1  1  1  1  1  1  1  1  1  1  1 10  1  1]\n",
      "Random Forest R-squared test score: 0.5555555555555556\n",
      "Random Forest R-squared score (training): 0.975000000\n"
     ]
    }
   ],
   "source": [
    "clf3 = RandomForestClassifier(max_features = 10, random_state = 0)\n",
    "Random_forest=clf3.fit(X_train, y_train)\n",
    "\n",
    "print(Random_forest.predict(X_test))\n",
    "print('Random Forest R-squared test score: {}'\n",
    "     .format(Random_forest.score(X_test, y_test)))\n",
    "print('Random Forest R-squared score (training): {:.9f}'\n",
    "     .format(Random_forest.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56451613  0.6         0.56034483]\n",
      "[ 0.54545455  0.5862069   0.57142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_Random_forest_train = cross_val_score(Random_forest, X_train, y_train, cv = 3)\n",
    "print(scores_Random_forest_train)\n",
    "scores_Random_forest_test = cross_val_score(Random_forest, X_test, y_test, cv = 3)\n",
    "print(scores_Random_forest_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score for training set: 0.574953652\n",
      "Average cross-validation score for testing set: 0.567696671\n"
     ]
    }
   ],
   "source": [
    "print(\"Average cross-validation score for training set: {:.9f}\".format(scores_Random_forest_train.mean()))\n",
    "print(\"Average cross-validation score for testing set: {:.9f}\".format(scores_Random_forest_test.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In dimensionality reduction, we have implemented PCA and manifold learning. We have reduced the variables to 83 components and have used all the models again. Based on the above regressions, we found following results:\n",
    "Logistic Regression, KNN, and linear SVM show the best results of 69%, 56.7% and 65% r squared values. We can say that the results werent too different from the original models where we did not reduce the number of components. This could be due to the fact that there was very less correlation in the data already.  \n",
    "\n",
    "Conclusion:\n",
    "Bagging can be considered as a good technique in determining the results. We can say this based on the r squared values obtained for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
